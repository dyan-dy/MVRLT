from typing import Dict, Tuple, List
import torch
from torch.utils.data import DataLoader
import numpy as np
from easydict import EasyDict as edict
import copy
import wandb

import utils3d.torch
from ..basic import BasicTrainer
from .structured_latent_vae_gaussian import SLatVaeGaussianTrainer
from ...representations import Gaussian
from ...renderers import GaussianRenderer
from ...modules.sparse import SparseTensor
from ...utils.loss_utils import l1_loss, l2_loss, ssim, lpips

class DecoderFinetuneTrainer(SLatVaeGaussianTrainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.cached_feats = None  # ç”¨æ¥ç¼“å­˜ç¬¬ä¸€æ¬¡å˜æ¢ç»“æœ

        self.latent_projector = torch.nn.Linear(
            in_features=1024,  # æ›¿æ¢æˆä½ å®é™…çš„è¾“å…¥ç»´åº¦
            out_features=768  # æ›¿æ¢æˆ decoder éœ€è¦çš„ç»´åº¦
        ).to(self.device)

        # å†»ç»“ encoder æ‰€æœ‰å‚æ•°
        if 'encoder' in self.models:
            for p in self.models['encoder'].parameters():
                p.requires_grad = False

            # # ä» training_models ä¸­ç§»é™¤ encoderï¼Œé¿å…å…¶å‡ºç°åœ¨ optimizer å‚æ•°ä¸­
            # if 'encoder' in self.training_models:
            #     del self.training_models['encoder']

        # ç¡®ä¿ decoder æ¨¡å‹çš„å‚æ•°èƒ½è¿›è¡Œè®­ç»ƒ
        self.training_models = {'encoder':self.models['encoder'], 'decoder': self.models['decoder']}
        self._enable_decoder_gradients()  # å¯ç”¨ decoder æ¢¯åº¦

    def _enable_decoder_gradients(self):
        """ç¡®ä¿ decoder æ¨¡å‹çš„æ‰€æœ‰å‚æ•°éƒ½éœ€è¦æ¢¯åº¦"""
        for name, param in self.models['decoder'].named_parameters():
            param.requires_grad = True  # ç¡®ä¿ decoder çš„æ‰€æœ‰å‚æ•°éƒ½æœ‰æ¢¯åº¦


    def training_losses(
        self,
        feats2: SparseTensor,
        image_light: torch.Tensor,
        image_env: torch.Tensor,
        # alpha: torch.Tensor,
        extrinsics: torch.Tensor,
        intrinsics: torch.Tensor,
        return_aux: bool = False,
        **kwargs
    ) -> Tuple[Dict, Dict]:
        """
        ä½¿ç”¨ç›´æ¥æä¾›çš„ latent feats1ï¼Œè®­ç»ƒ decoderã€‚
        """
        # breakpoint()
        # print("ğŸ‚ feats1 type", type(feats1))
        # z = feats1.feats.requires_grad_()  # feats1 ç›´æ¥ä½œä¸º latentï¼Œä¸éœ€è¦é€šè¿‡ encoder

        if self.cached_feats is None:
            if hasattr(feats2, 'feats'):  # æ”¯æŒ SparseTensor
                raw_feats = feats2.feats
            else:
                raw_feats = feats2
            z = self.latent_projector(raw_feats)
            self.cached_feats = z.detach().clone()  # ç¼“å­˜ç¬¬ä¸€æ¬¡å˜æ¢ç»“æœ
        else:
            z = self.cached_feats

        # z, mean, logvar = self.training_models['encoder'](feats2, sample_posterior=True, return_raw=True)
        # print("z.shape", z.shape)
        reps = self.training_models['decoder'](z)
        self.renderer.rendering_options.resolution = image_env.shape[-1]
        render_results = self._render_batch(reps, extrinsics, intrinsics)

        device = image_env.device
        # terms = edict(loss=0.0, rec=0.0)
        terms = edict(
            loss=torch.tensor(0.0, device=device, requires_grad=True),
            rec=torch.tensor(0.0, device=device, requires_grad=True)
        )
        # terms = edict()

        rec_image = render_results['color']
        # gt_image = image * alpha[:, None] + (1 - alpha[:, None]) * render_results['bg_color'][..., None, None]
        gt_image = image_env

        # print("terms[loss] requires_grad:", terms["loss"].requires_grad)
        # print("terms[loss] grad_fn:", terms["loss"].grad_fn)
        # breakpoint()
        if self.loss_type == 'l1':
            print("loss_type is l1")
            terms["l1"] = l1_loss(rec_image, gt_image)
            terms["rec"] = terms["rec"] + terms["l1"]
        elif self.loss_type == 'l2':
            print("loss_type is l2")
            terms["l2"] = l2_loss(rec_image, gt_image)
            terms["rec"] = terms["rec"] + terms["l2"]
        else:
            raise ValueError(f"Invalid loss type: {self.loss_type}")
        if self.lambda_ssim > 0:
            print("lambda ssim", self.lambda_ssim)
            terms["ssim"] = 1 - ssim(rec_image, gt_image)
            terms["rec"] = terms["rec"] + self.lambda_ssim * terms["ssim"]
        if self.lambda_lpips > 0:
            print("lambda lpips", self.lambda_lpips)
            terms["lpips"] = lpips(rec_image, gt_image)
            terms["rec"] = terms["rec"] + self.lambda_lpips * terms["lpips"]
        terms["loss"] = terms["loss"] + terms["rec"]

        # ä¸å†è®¡ç®— KL loss
        terms["kl"] = torch.tensor(0.0, device=image_env.device)

        # Regularization
        reg_loss, reg_terms = self._get_regularization_loss(reps)
        terms.update(reg_terms)
        terms["loss"] = terms["loss"] + reg_loss

        status = self._get_status(z, reps)

        # ===== wandb logging (æ¯ä¸ª loss é¡¹éƒ½è®°å½•) =====
        print("âœ recodring loss ... ", terms.items())
        wandb_log = {}
        for key, value in terms.items():
            if isinstance(value, torch.Tensor):
                wandb_log[f"loss/{key}"] = value.item()

        # è®°å½•å›¾åƒï¼ˆbatch ä¸­ç¬¬ 0 å¼ ï¼‰
        wandb_log["image/rec"] = wandb.Image(rec_image[0].clamp(0, 1))
        wandb_log["image/gt"] = wandb.Image(gt_image[0].clamp(0, 1))

        wandb.log(wandb_log)

        if return_aux:
            return terms, status, {'rec_image': rec_image, 'gt_image': gt_image}
        return terms, status

